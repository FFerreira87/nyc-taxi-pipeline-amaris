NYC Taxi Data Pipeline - Senior Data Engineering Challenge
Este repositÃ³rio contÃ©m uma soluÃ§Ã£o completa de engenharia de dados para o processamento dos dados de viagens dos tÃ¡xis de Nova Iorque (NYC TLC). A arquitetura segue o padrÃ£o Medallion (Bronze, Silver, Gold) e utiliza Terraform para provisionamento de infraestrutura na GCP.

ğŸ— Arquitetura da SoluÃ§Ã£o
A soluÃ§Ã£o foi desenhada para ser escalÃ¡vel e resiliente, utilizando:

Armazenamento: Google Cloud Storage (GCS) como Data Lake.

Processamento: PySpark para transformaÃ§Ãµes distribuÃ­das.

Data Warehouse: BigQuery para a camada Gold (Modelo Dimensional).

IaC: Terraform para gestÃ£o de recursos e IAM.

Fluxo de Dados:
Bronze (Ingestion): IngestÃ£o de arquivos brutos (Parquet/CSV) com adiÃ§Ã£o de metadados de auditoria.

Silver (Processing): Limpeza de dados, filtragem de anomalias (valores negativos/zero) e enriquecimento via Broadcast Join com a tabela de zonas.

Gold (Curated): Modelagem Dimensional (Star Schema) com tabelas particionadas e clusterizadas no BigQuery.

ğŸ› ï¸ Tecnologias Utilizadas
Python / PySpark

Terraform (Provider: Google Cloud)

Google Cloud Platform (GCS e BigQuery)

Parquet (Formato de armazenamento otimizado)

ğŸ“‚ Estrutura do Projeto
Plaintext

â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bronze_ingestion.py      # Job 1: Raw to Bronze
â”‚   â”œâ”€â”€ silver_transformation.py # Job 2: Bronze to Silver (Cleaning & Join)
â”‚   â””â”€â”€ gold_transformation.py   # Job 3: Silver to Gold (Dimensional Modeling)
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf                 # Recursos GCP (Storage, BQ, IAM)
â”‚   â””â”€â”€ variables.tf            # ParametrizaÃ§Ã£o do ambiente
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ diagram.png             # Desenho da arquitetura
â””â”€â”€ README.md

ğŸš€ Como Executar
1. Infraestrutura (Terraform)
Para provisionar o ambiente na GCP:

Bash

cd terraform
terraform init
terraform plan
terraform apply

2. Pipeline de Dados (Local)
Certifique-se de ter o pyspark instalado e o JAVA_HOME configurado.

Bash

# Execute os jobs na ordem da arquitetura medalhÃ£o
python scripts/bronze_ingestion.py
python scripts/silver_transformation.py
python scripts/gold_transformation.py

ğŸ“ˆ DecisÃµes de Engenharia (VisÃ£o SÃªnior)
Modelagem Dimensional (Gold)
Fact_Trips: Particionada por pickup_date para otimizar consultas temporais e reduzir custos de scan no BigQuery.

Dim_Zones: Criada a partir do cruzamento com o Zone Lookup para facilitar a anÃ¡lise por bairro (Borough) sem necessidade de joins custosos em tempo de execuÃ§Ã£o de BI.

Performance & OtimizaÃ§Ã£o
UtilizaÃ§Ã£o de Broadcast Join na camada Silver para a tabela de zonas, visto que Ã© uma tabela pequena (Lookup), evitando o shuffle de rede no Spark.

Uso do formato Parquet com compressÃ£o Snappy para reduzir o footprint de armazenamento e acelerar a leitura de colunas especÃ­ficas.

EstratÃ©gia de Dados HistÃ³ricos
A pipeline foi construÃ­da utilizando o conceito de IdempotÃªncia. Os jobs de escrita utilizam o modo overwrite baseado em partiÃ§Ãµes. Isso permite que qualquer perÃ­odo histÃ³rico possa ser reprocessado manualmente apenas re-executando o job para o arquivo de origem correspondente, sem gerar duplicidade no Data Warehouse.

ğŸ‘¤ Autor
Fabio M Ferreira - Senior Data Engineer Candidate